# Flying Picker â€” Vision System

A computer-vision pipeline that detects a rectangular object on a conveyor belt, extracts its **position (x, y)** and **orientation (angle)**, and displays the results in a live overlay window. Designed as the "eyes" of a robotic pick-and-place system.

---

## Table of Contents

- [Overview](#overview)
- [Project Structure](#project-structure)
- [Architecture](#architecture)
- [How It Works â€” Step by Step](#how-it-works--step-by-step)
- [Getting Started](#getting-started)
- [Configuration](#configuration)
- [Calibration Steps (When Hardware Is Ready)](#calibration-steps-when-hardware-is-ready)
- [What Has Been Done](#what-has-been-done)
- [What Still Needs to Be Done](#what-still-needs-to-be-done)

---

## Overview

The **Flying Picker** is a robotic pick-and-place system. A camera watches a conveyor belt, detects objects, and sends their position + angle to a robot so it can pick them mid-flight.

### Target Hardware

| # | Item | Purpose | Est. Cost |
|---|------|---------|----------|
| 1 | **Raspberry Pi 5 (8 GB RAM)** | Compute â€” runs the vision pipeline | â‚¬99 |
| 2 | Official USB-C Power Supply (27 W) | Power â€” Pi 5 requires the official 27 W PD supply | â‚¬20.20 |
| 3 | Active Cooler for Pi 5 | Cooling â€” prevents thermal throttling during vision processing | â‚¬10.10 |
| 4 | **Raspberry Pi Global Shutter Camera (Sony IMX296)** | The Eye â€” global shutter eliminates motion blur / jelly effect on moving conveyor | â‚¬85 |
| 5 | 6 mm CS-Mount Lens | The Optic â€” wide-angle lens to see the full belt width | â‚¬30 |
| 6 | Camera Cable (Mini â†’ Standard) | Pi 5 uses a Mini CSI connector; adapter cable needed | â‚¬3 |
| 7 | MicroSD Card (32 GB, A2 Class) | Storage â€” fast boot + program loading | â‚¬25.30 |
| 8 | LED Strip (12 V Cool White) | Lighting â€” consistent illumination for reliable detection | â‚¬12.99 |
| | | **Total** | **â‚¬285.59** |

Currently the vision system runs on a **pre-recorded video** (`public/IMG_6256.MOV`) of a light-coloured square card on a dark surface. No camera hardware is needed yet â€” the architecture is designed so the Pi Global Shutter Camera can be swapped in later (via `picamera2`) without changing the detection logic.

---

## Project Structure

```
flying-picker-v1/
â”‚
â”œâ”€â”€ run_vision.py                  # â† Entry point â€” run the detection pipeline
â”œâ”€â”€ run_camera_calibration.py      # Calibrate lens distortion (checkerboard)
â”œâ”€â”€ run_homography_calibration.py  # Calibrate pixel â†’ world mm transform
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ vision_config.yaml         # All tunable parameters (thresholds, colors, paths)
â”‚   â”œâ”€â”€ camera_calibration.json    # (auto-generated by run_camera_calibration.py)
â”‚   â””â”€â”€ homography.json            # (auto-generated by run_homography_calibration.py)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ vision/
â”‚   â”‚   â”œâ”€â”€ __init__.py            # Package exports
â”‚   â”‚   â”œâ”€â”€ frame_source.py        # FrameSource class â€” reads frames from video/camera
â”‚   â”‚   â”œâ”€â”€ preprocess.py          # Grayscale â†’ blur â†’ threshold â†’ morphology
â”‚   â”‚   â”œâ”€â”€ detection.py           # Contour detection â†’ minAreaRect â†’ (x, y, angle)
â”‚   â”‚   â”œâ”€â”€ calibration.py         # Camera intrinsic calibration (undistortion)
â”‚   â”‚   â”œâ”€â”€ coordinate_transform.py # Pixelâ†’world homography + belt compensation
â”‚   â”‚   â””â”€â”€ pipeline.py            # Main loop â€” ties everything together + live overlay
â”‚   â”‚
â”‚   â””â”€â”€ robot/                     # (empty â€” future robot communication module)
â”‚
â”œâ”€â”€ tests/                         # (empty â€” future unit/integration tests)
â”‚
â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ IMG_6256.MOV               # Sample video of the object on a surface
â”‚   â”œâ”€â”€ 01_Flying picker.pdf       # Project overview document
â”‚   â”œâ”€â”€ Flying Picker research paper.pdf
â”‚   â””â”€â”€ Vision_System_BOM.pdf      # Bill of Materials for vision hardware
â”‚
â””â”€â”€ requirements.txt               # Python dependencies
```

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        run_vision.py                             â”‚
â”‚                      (entry point)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     pipeline.py                                  â”‚
â”‚              (main loop â€” orchestrator)                          â”‚
â”‚                                                                  â”‚
â”‚   Loads config + calibration files (if they exist)               â”‚
â”‚   Opens FrameSource â”€â”€â†’ Loop:                                    â”‚
â”‚                           â”‚                                      â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚
â”‚        â”‚                                                         â”‚
â”‚        â–¼                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚ FrameSource â”‚â”€â”€â–¶â”‚ undistort()  â”‚â”€â”€â–¶â”‚    ROI crop      â”‚     â”‚
â”‚   â”‚             â”‚   â”‚ (if calib    â”‚   â”‚ (configurable    â”‚     â”‚
â”‚   â”‚ Read next   â”‚   â”‚  exists)     â”‚   â”‚  edge removal)   â”‚     â”‚
â”‚   â”‚ video frame â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚   â”‚ (loops at   â”‚                              â”‚                â”‚
â”‚   â”‚  end of     â”‚                              â–¼                â”‚
â”‚   â”‚  video)     â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ preprocess() â”‚â”€â”€â–¶â”‚ detect_object()  â”‚     â”‚
â”‚                     â”‚              â”‚   â”‚                  â”‚     â”‚
â”‚                     â”‚ BGRâ†’Gray     â”‚   â”‚ Find contours   â”‚     â”‚
â”‚                     â”‚ Gaussian blurâ”‚   â”‚ Largest contour â”‚     â”‚
â”‚                     â”‚ Binary threshâ”‚   â”‚ minAreaRect()   â”‚     â”‚
â”‚                     â”‚ Morph close  â”‚   â”‚ â†’ (x, y, Î¸) px  â”‚     â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                â”‚                â”‚
â”‚                                                â–¼                â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚                     â”‚  compensate  â”‚â—€â”€â”€â”‚ pixel_to_world() â”‚     â”‚
â”‚                     â”‚  _belt       â”‚   â”‚ (if homography   â”‚     â”‚
â”‚                     â”‚  _motion()   â”‚   â”‚  exists)         â”‚     â”‚
â”‚                     â”‚ (if belt     â”‚   â”‚ â†’ (X, Y, Î¸) mm  â”‚     â”‚
â”‚                     â”‚  enabled)    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                     â”‚ â†’ pick pos   â”‚                            â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                            â”‚                                    â”‚
â”‚                            â–¼                                    â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚                   â”‚ Draw overlay   â”‚                            â”‚
â”‚                   â”‚ â€¢ Green box    â”‚                            â”‚
â”‚                   â”‚ â€¢ Red centroid â”‚                            â”‚
â”‚                   â”‚ â€¢ mm or px textâ”‚                            â”‚
â”‚                   â”‚ cv2.imshow()   â”‚                            â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

       â–²                           â–²                       â–²
       â”‚ config                    â”‚ auto-loaded           â”‚ auto-loaded
       â”‚                           â”‚ if file exists        â”‚ if file exists
â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ vision_      â”‚   â”‚ camera_calibration    â”‚   â”‚ homography.json    â”‚
â”‚ config.yaml  â”‚   â”‚ .json                 â”‚   â”‚                    â”‚
â”‚              â”‚   â”‚                       â”‚   â”‚ Pixel â†’ mm         â”‚
â”‚ thresholds   â”‚   â”‚ Camera matrix +       â”‚   â”‚ transform matrix   â”‚
â”‚ ROI, belt    â”‚   â”‚ distortion coeffs     â”‚   â”‚                    â”‚
â”‚ display opts â”‚   â”‚ (from checkerboard)   â”‚   â”‚ (from ref points)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The pipeline **gracefully degrades** â€” without calibration files it works in pixel mode (exactly like before). Drop in `camera_calibration.json` â†’ undistortion kicks in. Drop in `homography.json` â†’ output switches to mm. Enable belt config â†’ pick-position prediction appears.

---

## How It Works â€” Step by Step

### 1. Frame Acquisition (`frame_source.py`)

`FrameSource` wraps OpenCV's `VideoCapture`. It opens the video file, reads one frame at a time, and **loops** back to frame 0 when the video ends (so you get a continuous stream for testing).

```
Video file (.MOV) â”€â”€â–¶ FrameSource.read() â”€â”€â–¶ BGR frame (numpy array)
```

### 2. Preprocessing (`preprocess.py`)

Each raw BGR frame goes through four steps to produce a clean **binary mask**:

| Step | Function | Why |
|------|----------|-----|
| **Grayscale** | `cv2.cvtColor(BGR2GRAY)` | Reduce 3 channels to 1 |
| **Gaussian Blur** | `cv2.GaussianBlur(kernel=5)` | Remove noise / smooth edges |
| **Binary Threshold** | `cv2.threshold(150)` | Light card â†’ white (255), dark background â†’ black (0) |
| **Morphological Close** | `cv2.morphologyEx(MORPH_CLOSE)` | Fill small holes/gaps inside the object |

**Input:** full-colour frame â†’ **Output:** binary mask (white object on black)

### 3. Detection (`detection.py`)

The binary mask is analysed to find the object's pose:

| Step | Function | Output |
|------|----------|--------|
| Find all contours | `cv2.findContours()` | List of contour point arrays |
| Pick the largest | `max(contours, key=cv2.contourArea)` | Single contour |
| Filter by min area | Area check (default 5000 pxÂ²) | Rejects noise |
| Fit rotated rectangle | `cv2.minAreaRect()` | Center (x, y), size (w, h), angle (Î¸) |
| Get box corners | `cv2.boxPoints()` | 4 corner coordinates for drawing |

Returns a `DetectionResult` dataclass:
```
DetectionResult(
    center_x,      # centroid X in pixels
    center_y,      # centroid Y in pixels
    width,          # bounding box width
    height,         # bounding box height
    angle,          # rotation in degrees
    contour,        # raw contour points
    box_points      # 4 corners of the rotated box
)
```

### 4. Overlay & Display (`pipeline.py`)

The pipeline draws on the original frame and shows it:

- **Green rotated bounding box** â€” the detected rectangle outline
- **Red dot** â€” centroid (x, y)
- **White text** â€” `x=... y=... angle=...Â°`

Two windows are shown:
- **"Flying Picker â€” Detection"** â€” original frame with overlay
- **"Flying Picker â€” Mask"** â€” the binary preprocessing result

Console output each frame:
```
[frame   42]  x=  330.4  y=  586.7  angle=   2.3Â°  (0.8 ms)
```

---

## Getting Started

### Prerequisites

- Python 3.10+
- macOS / Linux / Windows

### Install

```bash
cd flying-picker-v1

# Create virtual environment (if not already done)
python -m venv .venv
source .venv/bin/activate        # macOS/Linux
# .venv\Scripts\activate         # Windows

# Install dependencies
pip install -r requirements.txt
```

### Run

```bash
python run_vision.py
```

Two OpenCV windows will open showing the live detection. Press **`q`** to quit.

### Run with a custom config

```bash
python run_vision.py path/to/custom_config.yaml
```

### Calibrate (when hardware is ready)

```bash
# Step 1: Camera calibration (lens distortion) from saved checkerboard images
python run_camera_calibration.py --dir path/to/checkerboard_photos/ --board-cols 9 --board-rows 6

# ... or live capture (press SPACE to grab each image)
python run_camera_calibration.py --source 0 --num-captures 15

# Step 2: Homography calibration (pixel â†’ mm) â€” interactive click mode
python run_homography_calibration.py --interactive --source 0 --world-points "0,0 400,0 400,300 0,300"

# ... or from known pixel/world point pairs
python run_homography_calibration.py \
    --pixel-points "100,50 500,50 500,400 100,400" \
    --world-points "0,0 400,0 400,300 0,300"
```

Once calibration files exist in `config/`, the pipeline will automatically use them.

---

## Configuration

All tunable parameters are in `config/vision_config.yaml`:

```yaml
input:
  video_path: "public/IMG_6256.MOV"    # Video source

preprocess:
  blur_kernel_size: 5       # Gaussian blur kernel (must be odd)
  threshold_value: 100      # Binary threshold cutoff (0â€“255)
  threshold_max: 255        # Value for pixels above threshold

detection:
  min_contour_area: 5000    # Ignore contours smaller than this (pxÂ²)

roi:                          # Crop frame edges to remove noise
  enabled: true
  x_start: 0.05
  y_start: 0.05
  x_end: 0.95
  y_end: 0.90

calibration:                  # Checkerboard params for camera calibration
  board_cols: 9
  board_rows: 6
  square_size_mm: 25.0
  result_file: "config/camera_calibration.json"

homography:                   # Pixel â†’ world mm transform
  result_file: "config/homography.json"

belt:                         # Conveyor compensation
  enabled: false
  speed_mm_s: 0.0
  detection_to_pick_delay_s: 0.0
  direction: "x"              # "x" or "y"

display:
  show_windows: true
  show_mask: true
  overlay_color: [0, 255, 0]  # BGR green
  centroid_color: [0, 0, 255] # BGR red
  text_color: [255, 255, 255] # BGR white
```

**Tuning tips:**
- If the mask has noise (white specks in background) â†’ **increase** `threshold_value`
- If the object is missing parts in the mask â†’ **decrease** `threshold_value`
- If small contours are being detected as the object â†’ **increase** `min_contour_area`
- If edges of the frame cause false detections â†’ tighten the `roi` bounds

---

## What Has Been Done

### âœ… Vision Pipeline (complete for file-based input)

| Component | File | Status |
|-----------|------|--------|
| Frame source (video file) | `src/vision/frame_source.py` | âœ… Done |
| Preprocessing (threshold) | `src/vision/preprocess.py` | âœ… Done |
| Object detection (contour) | `src/vision/detection.py` | âœ… Done |
| Main pipeline + live overlay | `src/vision/pipeline.py` | âœ… Done |
| Configuration system | `config/vision_config.yaml` | âœ… Done |
| Entry point script | `run_vision.py` | âœ… Done |
| Dependencies | `requirements.txt` | âœ… Updated |

### âœ… Calibration & Coordinate Transform (code complete â€” ready for hardware)

| Component | File | Status |
|-----------|------|--------|
| Camera intrinsic calibration | `src/vision/calibration.py` | âœ… Done |
| Pixel â†’ world homography | `src/vision/coordinate_transform.py` | âœ… Done |
| Belt motion compensation | `src/vision/coordinate_transform.py` | âœ… Done |
| Camera calibration script | `run_camera_calibration.py` | âœ… Done |
| Homography calibration script | `run_homography_calibration.py` | âœ… Done |
| Pipeline integration (undistort + homography + belt) | `src/vision/pipeline.py` | âœ… Done |

### Current capabilities

- Reads video at native FPS (~30 fps)
- Detects a single light rectangular object on a dark background
- Extracts centroid (x, y) and rotation angle (Î¸) per frame
- Draws live overlay (bounding box + centroid + angle label)
- Loops video for continuous testing
- All parameters configurable via YAML (no code changes needed)
- ~1 ms processing time per frame
- **Auto-loads calibration files** if they exist (graceful degradation)
- **Pixel â†’ mm output** when homography is calibrated
- **Belt compensation** predicts pick position when belt config is enabled
- CLI scripts for camera calibration (from images or live) and homography calibration (interactive click or direct points)

---

## What Still Needs to Be Done

### Physical Measurements Required

Before calibration and robot integration can happen, we need to physically measure and record these values from the actual conveyor setup. **None of this can be done in software â€” it must be measured on-site.**

| # | Measurement | Example Value | Why It's Needed | How to Measure |
|---|-------------|---------------|-----------------|----------------|
| 1 | **Conveyor belt width** | e.g. 400 mm | Defines the X working area. Homography maps pixels â†’ mm across this width. | Tape measure across the belt |
| 2 | **Conveyor belt speed** | e.g. 200 mm/s | Object moves between detection and robot pick. Robot must predict where the object *will be*. | Encoder on belt motor, or time a marked point over a known distance |
| 3 | **Camera height above belt** | e.g. 500 mm | Determines field of view and mm-per-pixel ratio. Higher = wider view, less resolution. | Measure from lens to belt surface |
| 4 | **Camera position relative to robot origin** | e.g. (Î”X=150, Î”Y=0) mm | Robot's (0,0) â‰  camera's (0,0). Need an offset so coordinates make sense to the robot. | Measure or calibrate with a known point visible to both camera and robot |
| 5 | **Object dimensions** | e.g. 80Ã—80 mm | Sanity check â€” compare detected size in mm to expected size to verify calibration accuracy. | Ruler on the object |
| 6 | **Detection-to-pick delay** | e.g. 50 ms | Time between "camera sees object" and "robot reaches object". Needed for belt compensation. | Measure or estimate from robot speed + communication latency |

These values will be stored in `config/vision_config.yaml` under a `calibration:` section once measured.

---

### Calibration Steps (When Hardware Is Ready)

#### Step 1: Camera Calibration (Intrinsic) â€” Lens Distortion Correction

The 6 mm CS-mount lens distorts the image (barrel/pincushion distortion). Without correcting this, (x, y) measurements are inaccurate near the frame edges.

**Procedure:**
1. Print a **checkerboard pattern** (e.g. 9Ã—6 inner corners) on A4 paper
2. Place it on the conveyor belt
3. Take **15â€“20 photos** from the camera at different angles/positions
4. Run the calibration script â†’ computes **camera matrix** + **distortion coefficients**
5. Save to `config/camera_calibration.json`
6. Pipeline applies `cv2.undistort()` on every frame before detection

**Math â€” what the camera matrix looks like:**
```
Camera Matrix K:
â”Œ fx   0   cx â”       fx, fy = focal lengths (pixels)
â”‚  0  fy   cy â”‚       cx, cy = principal point (image center)
â””  0   0    1 â”˜

Distortion Coefficients:
[k1, k2, p1, p2, k3]   (radial + tangential distortion)
```

**Code:** `src/vision/calibration.py` â†’ `calibrate_from_images()` / `calibrate_live()` âœ…
**Script:** `python run_camera_calibration.py --dir path/to/checkerboard_images/` âœ…

---

#### Step 2: Pixel â†’ World Coordinate Transform (Homography)

Currently the pipeline outputs positions in **pixels** (e.g. `x=332, y=585`). The robot needs positions in **millimetres on the conveyor belt** (e.g. `X=87.5 mm from left edge, Y=210.3 mm from trigger line`).

**Procedure:**
1. Place **4 or more markers** at known positions on the conveyor belt (e.g. corners of a rectangle with known mm dimensions)
2. Record their pixel positions from the camera
3. Compute the **homography matrix H** using `cv2.findHomography(pixel_pts, world_pts)`
4. Save H to `config/homography.json`
5. Pipeline applies the transform to every detection

**Math â€” homography transform:**
```
â”Œ Xw â”       â”Œ h11  h12  h13 â”   â”Œ xp â”
â”‚ Yw â”‚ = H â‹… â”‚ h21  h22  h23 â”‚ â‹… â”‚ yp â”‚
â””  1 â”˜       â”” h31  h32  h33 â”˜   â””  1 â”˜

Where:
  (xp, yp) = pixel coordinates from detection
  (Xw, Yw) = real-world coordinates in mm on the conveyor
  H         = 3Ã—3 homography matrix (computed once during calibration)
```

**Example:**
| What you have now | What you'll have after calibration |
|---|---|
| `x = 332 px, y = 585 px` | `X = 87.5 mm, Y = 210.3 mm` (from belt edge/trigger line) |
| `angle = 14.1Â°` | `angle = 14.1Â°` (already in degrees â€” no conversion needed) |

**Code:** `src/vision/coordinate_transform.py` â†’ `pixel_to_world()` âœ…
**Script:** `python run_homography_calibration.py --interactive --source <video> --world-points "x1,y1 ..."` âœ…

---

#### Step 3: Conveyor Belt Compensation

The object is **moving** on the belt. Between the moment the camera detects it and the moment the robot reaches it, the object has moved. The robot must aim at the **predicted future position**.

**Math:**
```
X_pick = X_detected + V_belt Ã— Î”t

Where:
  X_detected = position at time of detection (mm, from homography)
  V_belt     = conveyor belt speed (mm/s, from encoder or config)
  Î”t         = detection-to-pick delay (seconds)
  X_pick     = where the robot should actually go
```

**Example:** Object detected at `X = 87.5 mm`. Belt moves at `200 mm/s`. Robot takes `50 ms` to reach. So: `X_pick = 87.5 + 200 Ã— 0.05 = 97.5 mm`.

**Code:** `src/vision/coordinate_transform.py` â†’ `compensate_belt_motion()` âœ…
**Config:** Set `belt.enabled: true`, `belt.speed_mm_s`, and `belt.detection_to_pick_delay_s` in `vision_config.yaml`

---

### Complete Pipeline After Calibration

```
Camera Frame
    â”‚
    â–¼
cv2.undistort()              â† Camera calibration (Step 1)
    â”‚
    â–¼
preprocess() â†’ mask
    â”‚
    â–¼
detect_object() â†’ (x_px, y_px, angle)
    â”‚
    â–¼
pixel_to_world(x_px, y_px)  â† Homography (Step 2)
    â”‚
    â–¼
(X_mm, Y_mm, angle)
    â”‚
    â–¼
compensate_belt_motion()     â† Belt speed + delay (Step 3)
    â”‚
    â–¼
(X_pick, Y_pick, angle)     â† Final output to robot
    â”‚
    â–¼
Send over serial to robot controller
```

---

### Implementation Roadmap

### ğŸ”² Phase 1 â€” Camera Integration (Raspberry Pi 5)

| Task | Details | Status |
|------|--------|--------|
| **Pi Global Shutter Camera capture** | Implement a `PiCameraSource` class using `picamera2` that matches the `FrameSource` interface. The Sony IMX296 global shutter sensor eliminates motion blur on the moving conveyor. Swap it into the pipeline via config. | ğŸ”² Needs hardware |
| **Camera calibration code** | Intrinsic calibration (lens distortion correction for the 6 mm CS-mount lens) using a checkerboard pattern + `cv2.calibrateCamera()`. Store results in `config/camera_calibration.json`. | âœ… Code done |
| **Run camera calibration** | Print checkerboard, take photos from Pi camera, run `python run_camera_calibration.py`. | ğŸ”² Needs hardware |
| **Pi 5 optimisation** | Ensure the pipeline runs efficiently on Pi 5's ARM CPU. Profile memory usage (8 GB available) and consider GPU acceleration via OpenCV's UMat if needed. | ğŸ”² Needs hardware |

### âœ… Phase 2 â€” Coordinate Mapping (code complete)

| Task | Details | Status |
|------|--------|--------|
| **Homography calibration tool** | `run_homography_calibration.py` â€” interactive (click reference points) or direct (pass point pairs via CLI). | âœ… Done |
| **`pixel_to_world()` function** | Apply the homography to convert every detection from pixels to mm. | âœ… Done |
| **Belt compensation** | `compensate_belt_motion()` â€” predict pick position using belt speed and detection-to-pick delay. | âœ… Done |
| **Pipeline integration** | Pipeline auto-loads calibration + homography files, shows mm output, predicts pick position. | âœ… Done |
| **Run homography calibration** | Place reference markers on belt, run calibration script with camera. | ğŸ”² Needs hardware |
| **Conveyor speed input** | Either read from an encoder (via serial/GPIO) or set as a constant in config. | ğŸ”² Needs hardware |

### ğŸ”² Phase 3 â€” Robot Communication

| Task | Details |
|------|---------|
| **Serial protocol** | Define the message format for sending `(X_mm, Y_mm, Î¸)` to the robot controller over `pyserial`. |
| **`src/robot/` module** | Implement serial connection, message packaging, and send/receive logic. |
| **Timing / sync** | Measure the actual detection-to-pick delay and tune the belt compensation. |

### ğŸ”² Phase 4 â€” Robustness & Testing

| Task | Details |
|------|---------|
| **Multi-object detection** | Extend `detect_object()` to return a list of results instead of just the largest. |
| **Adaptive thresholding** | Handle varying lighting conditions (e.g., `cv2.adaptiveThreshold` or HSV-based segmentation). |
| **Unit tests** | Add tests in `tests/` for preprocessing, detection, and coordinate transforms. |
| **Logging** | Replace `print()` with Python `logging` module for configurable verbosity. |
| **Error handling** | Graceful recovery from dropped frames, serial disconnects, etc. |

### ğŸ”² Phase 5 â€” Documentation & Polish

| Task | Details |
|------|---------|
| **Calibration guide** | Step-by-step instructions with photos for camera + homography calibration. |
| **Deployment guide** | How to run on the actual conveyor setup with the Pi 5. |
| **Performance profiling** | Benchmark end-to-end latency under real conditions. |

---

## Dependencies

| Package | Version | Purpose |
|---------|---------|---------|
| `numpy` | latest | Array operations |
| `opencv-python` | latest | Computer vision (capture, threshold, contours, display) |
| `pyyaml` | â‰¥6.0 | Configuration file loading |
| `picamera2` | â€” | Raspberry Pi camera library *(commented out â€” enable when running on Pi 5)* |
| `pyserial` | â€” | Robot serial communication *(commented out â€” enable when hardware available)* |

---

## License

*(To be determined)*
